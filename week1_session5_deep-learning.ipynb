{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Getting started with Deep Learning"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Tutors: Fabian Eitel (Fabian.Eitel@charite.de) and Talia Kimber (talia.kimber@charite.de)"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 1. Aims of this session\n\nGet a rough idea of how artificial neural networks (ANNs) work, how an implementation in Keras looks like and how suitable they are for tabular data."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# Learning goals\n\n\n## Theory\n\n* Building blocks of ANNs\n* Model training\n\n## Practical\n\n* Learn to understand the basics using the Tensorflow playground\n* Learn to read a model definition in Python using Keras\n* Run a pipeline of an ANN on the ADNI tabular data\n* Investigate what filters learn at different layers"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# References\n\n* Stanford Course on Deep Learning http://cs231n.github.io/"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Theory\n\n\n### Building blocks of artificial neural networks\nShowing some of the blocks that can be used when training neural networks and some widely used examples.\n\n__Layer types__\n* Fully connected/linear/dense layers\n* Convolutional layers\n* Pooling layers and other down/upsampling layers\n* Utility layers like input and output layers\n* Batch normalization\n\n__Activation types__\n* Sigmoid\n* Linear\n* Tanh\n* ReLU\n* Leaky ReLU and other variants\n\n__Regularizers__\n* L1 regularization (used in LASSO)\n* L2 regularization / almost the same as weight decay (used in Ridge regression)\n* Dropout\n* Early stopping\n\n__Data functions__\n* Normalization (e.g. using mean and standard deviation)\n* Data augmentation\n* Feature reduction (e.g. Principal Component Analysis [PCA])\n\n__Cost functions__\nCost functions depend on your type of analysis, i.e. regression, binary classification, multi-class classification etc.\n* Softmax\n* Cross-entropy\n* Binary cross-entropy\n* Kullback-Leibler Divergence\n* Smooth losses\n* Mean-squared error\n\nFor more information on each topic view the course link in the references."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "https://scs.ryerson.ca/~aharley/vis/conv/"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "https://www.youtube.com/watch?v=AgkfIQ4IGaM"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "# 2. Playground excersises"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__Introduction__\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "https://playground.tensorflow.org"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Tensorflow playground is a neural network framework you can use in your browser. Unlike the name says its not based on the popular Tensorflow program. It allows to get some intuition on neural network workings."
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__2.1 Exercise__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Use the XOR dataset with 1 hidden layer and try out different loss functions:\n\nhttps://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4&seed=0.82689&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__2.2 Exercise__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What happens if you add more features one by one?\nStart with X12. Maybe you can add extra layers and neurons too.\n\nhttps://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4&seed=0.82689&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=true&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__2.3 Exercise__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let's try a different dataset. Investigate the effects of the learning rate on the training results:\n\nhttps://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=4,2&seed=0.19504&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Real data is never this clean, it is usually were noisy. Now, use the same model from above and add some noise to the data distribution (middle slider on the bottom left). How does it affect the data (you see it on the right) and your model performance?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__2.4 Exercise__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "After you have added the noise, try out L1 and L2 regularization. When does it help?"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__2.5 Exercise__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "What does it mean for a model to _converge_? Here is an example where it does not converge. Can you fix it?\n\nhttps://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.3&regularizationRate=0&noise=25&networkShape=4,2&seed=0.84469&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "__2.6 Exercise__"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Use everything you have learned so far on the more challenging spiral data:\n\nhttps://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=25&networkShape=4,2&seed=0.07992&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\n\nCan you beat my quick experiments?\n\nhttps://playground.tensorflow.org/#activation=relu&regularization=L2&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0.03&noise=25&networkShape=5,4,2&seed=0.16124&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification&initZero=false&hideText=false"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Practical part"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Import required packages\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.metrics import balanced_accuracy_score",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Load data table\ndf = pd.read_csv(\"data/alzheimers_disease_rand.csv\")\n# Print first 5 rows\ndf.head()",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2728: DtypeWarning: Columns (16,17,18,99,100) have mixed types. Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n",
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "execution_count": 2,
          "data": {
            "text/plain": "   RID VISCODE  SITE    EXAMDATE DX_bl   AGE PTGENDER  PTEDUCAT  \\\n0  128      bl   164  2005-09-08    CN  74.2     Male        16   \n1  129      bl   164  2005-09-12    AD  82.4     Male        18   \n2  129     m06   164  2006-03-13    AD  81.4     Male        18   \n3  129     m12   164  2006-09-12    AD  81.3     Male        18   \n4  129     m24   164  2007-09-12    AD  80.5     Male        18   \n\n                          WORK         PTETHCAT      ...      PTAU_bl  \\\n0  technical writer and editor  Not Hisp/Latino      ...          NaN   \n1                    Secretary  Not Hisp/Latino      ...        22.83   \n2    Elementary school teacher  Not Hisp/Latino      ...        22.83   \n3                Communication  Not Hisp/Latino      ...        22.83   \n4                   Accounting  Not Hisp/Latino      ...        22.83   \n\n    FDG_bl PIB_bl  AV45_bl  Years_bl  Month_bl Month     M  \\\n0  1.36665    NaN      NaN  0.000000   0.00000   0.0   0.0   \n1  1.08355    NaN      NaN  0.000000   0.00000   0.0   0.0   \n2  1.08355    NaN      NaN  0.498289   5.96721   6.0   6.0   \n3  1.08355    NaN      NaN  0.999316  11.96720  12.0  12.0   \n4  1.08355    NaN      NaN  1.998630  23.93440  24.0  24.0   \n\n            update_stamp  Unnamed: 109  \n0  2019-12-04 04:19:56.0           NaN  \n1  2019-12-04 04:19:56.0           NaN  \n2  2019-12-04 04:19:56.0           NaN  \n3  2019-12-04 04:19:56.0           NaN  \n4  2019-12-04 04:19:56.0           NaN  \n\n[5 rows x 110 columns]",
            "text/html": "<div>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RID</th>\n      <th>VISCODE</th>\n      <th>SITE</th>\n      <th>EXAMDATE</th>\n      <th>DX_bl</th>\n      <th>AGE</th>\n      <th>PTGENDER</th>\n      <th>PTEDUCAT</th>\n      <th>WORK</th>\n      <th>PTETHCAT</th>\n      <th>...</th>\n      <th>PTAU_bl</th>\n      <th>FDG_bl</th>\n      <th>PIB_bl</th>\n      <th>AV45_bl</th>\n      <th>Years_bl</th>\n      <th>Month_bl</th>\n      <th>Month</th>\n      <th>M</th>\n      <th>update_stamp</th>\n      <th>Unnamed: 109</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>128</td>\n      <td>bl</td>\n      <td>164</td>\n      <td>2005-09-08</td>\n      <td>CN</td>\n      <td>74.2</td>\n      <td>Male</td>\n      <td>16</td>\n      <td>technical writer and editor</td>\n      <td>Not Hisp/Latino</td>\n      <td>...</td>\n      <td>NaN</td>\n      <td>1.36665</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-12-04 04:19:56.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>129</td>\n      <td>bl</td>\n      <td>164</td>\n      <td>2005-09-12</td>\n      <td>AD</td>\n      <td>82.4</td>\n      <td>Male</td>\n      <td>18</td>\n      <td>Secretary</td>\n      <td>Not Hisp/Latino</td>\n      <td>...</td>\n      <td>22.83</td>\n      <td>1.08355</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.000000</td>\n      <td>0.00000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>2019-12-04 04:19:56.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>129</td>\n      <td>m06</td>\n      <td>164</td>\n      <td>2006-03-13</td>\n      <td>AD</td>\n      <td>81.4</td>\n      <td>Male</td>\n      <td>18</td>\n      <td>Elementary school teacher</td>\n      <td>Not Hisp/Latino</td>\n      <td>...</td>\n      <td>22.83</td>\n      <td>1.08355</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.498289</td>\n      <td>5.96721</td>\n      <td>6.0</td>\n      <td>6.0</td>\n      <td>2019-12-04 04:19:56.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>129</td>\n      <td>m12</td>\n      <td>164</td>\n      <td>2006-09-12</td>\n      <td>AD</td>\n      <td>81.3</td>\n      <td>Male</td>\n      <td>18</td>\n      <td>Communication</td>\n      <td>Not Hisp/Latino</td>\n      <td>...</td>\n      <td>22.83</td>\n      <td>1.08355</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>0.999316</td>\n      <td>11.96720</td>\n      <td>12.0</td>\n      <td>12.0</td>\n      <td>2019-12-04 04:19:56.0</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>129</td>\n      <td>m24</td>\n      <td>164</td>\n      <td>2007-09-12</td>\n      <td>AD</td>\n      <td>80.5</td>\n      <td>Male</td>\n      <td>18</td>\n      <td>Accounting</td>\n      <td>Not Hisp/Latino</td>\n      <td>...</td>\n      <td>22.83</td>\n      <td>1.08355</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.998630</td>\n      <td>23.93440</td>\n      <td>24.0</td>\n      <td>24.0</td>\n      <td>2019-12-04 04:19:56.0</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 110 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "list(df.keys())",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 3,
          "data": {
            "text/plain": "['RID',\n 'VISCODE',\n 'SITE',\n 'EXAMDATE',\n 'DX_bl',\n 'AGE',\n 'PTGENDER',\n 'PTEDUCAT',\n 'WORK',\n 'PTETHCAT',\n 'PTRACCAT',\n 'PTMARRY',\n 'APOE4',\n 'FDG',\n 'PIB',\n 'AV45',\n 'ABETA',\n 'TAU',\n 'PTAU',\n 'CDRSB',\n 'ADAS11',\n 'ADAS13',\n 'ADASQ4',\n 'MMSE',\n 'RAVLT_immediate',\n 'RAVLT_learning',\n 'RAVLT_forgetting',\n 'RAVLT_perc_forgetting',\n 'LDELTOTAL',\n 'DIGITSCOR',\n 'TRABSCOR',\n 'FAQ',\n 'MOCA',\n 'EcogPtMem',\n 'EcogPtLang',\n 'EcogPtVisspat',\n 'EcogPtPlan',\n 'EcogPtOrgan',\n 'EcogPtDivatt',\n 'EcogPtTotal',\n 'EcogSPMem',\n 'EcogSPLang',\n 'EcogSPVisspat',\n 'EcogSPPlan',\n 'EcogSPOrgan',\n 'EcogSPDivatt',\n 'EcogSPTotal',\n 'FLDSTRENG',\n 'IMAGEUID',\n 'Ventricles',\n 'Hippocampus',\n 'WholeBrain',\n 'Entorhinal',\n 'Fusiform',\n 'MidTemp',\n 'ICV',\n 'DX',\n 'mPACCdigit',\n 'mPACCtrailsB',\n 'EXAMDATE_bl',\n 'CDRSB_bl',\n 'ADAS11_bl',\n 'ADAS13_bl',\n 'ADASQ4_bl',\n 'MMSE_bl',\n 'RAVLT_immediate_bl',\n 'RAVLT_learning_bl',\n 'RAVLT_forgetting_bl',\n 'RAVLT_perc_forgetting_bl',\n 'LDELTOTAL_BL',\n 'DIGITSCOR_bl',\n 'TRABSCOR_bl',\n 'FAQ_bl',\n 'mPACCdigit_bl',\n 'mPACCtrailsB_bl',\n 'FLDSTRENG_bl',\n 'Ventricles_bl',\n 'Hippocampus_bl',\n 'WholeBrain_bl',\n 'Entorhinal_bl',\n 'Fusiform_bl',\n 'MidTemp_bl',\n 'ICV_bl',\n 'MOCA_bl',\n 'EcogPtMem_bl',\n 'EcogPtLang_bl',\n 'EcogPtVisspat_bl',\n 'EcogPtPlan_bl',\n 'EcogPtOrgan_bl',\n 'EcogPtDivatt_bl',\n 'EcogPtTotal_bl',\n 'EcogSPMem_bl',\n 'EcogSPLang_bl',\n 'EcogSPVisspat_bl',\n 'EcogSPPlan_bl',\n 'EcogSPOrgan_bl',\n 'EcogSPDivatt_bl',\n 'EcogSPTotal_bl',\n 'ABETA_bl',\n 'TAU_bl',\n 'PTAU_bl',\n 'FDG_bl',\n 'PIB_bl',\n 'AV45_bl',\n 'Years_bl',\n 'Month_bl',\n 'Month',\n 'M',\n 'update_stamp',\n 'Unnamed: 109']"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = df[df.VISCODE == \"m12\"]\ndf = df[df.DX != \"MCI\"]",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "df = df.dropna(subset=[\"Hippocampus\", \"DX\", \"Ventricles\"])",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Data splitting"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Get an array with the number of samples\nindices = np.arange(len(df))\nprint(\"Order before shuffling: %s\" % indices[:5])\n\n# Shuffle that array\nnp.random.seed(42) # fix a seed so each random event can be repeated\nnp.random.shuffle(indices)\nprint(\"Order after shuffling: %s\"  % indices[:5])",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Order before shuffling: [0 1 2 3 4]\nOrder after shuffling: [499 587 195 165 543]\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Take first 80% as a training set\nlen_training = int(len(indices) * 0.8) # use int() function to remove decimals\nprint(\"Number of samples for training set: %i\" % len_training)\n\n# Select the first 80% indices\ntrain_idx = indices[0:len_training] # pick 0 to the value of len_training from the indices array",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Number of samples for training set: 473\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Take the remaining data and split it 50/50\nremaining_samples = len(indices) - len_training\nlen_validation = int(np.ceil(remaining_samples/2)) # round up once\nlen_test = int(np.floor(remaining_samples/2)) # round down once\n\n# Select from the indices array the individual groups\nvalidation_idx = indices[len_training:len_training+len_validation]\ntest_idx = indices[len_training+len_validation:len(indices)]",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "print(\"Number of training samples: %i\" % len(train_idx))\nprint(\"Number of validation samples: %i\" % len(validation_idx))\nprint(\"Number of test samples: %i\" % len(test_idx))\nprint(\"Total number of samples: %i\" % (len(train_idx) + len(validation_idx) + len(test_idx)))",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "Number of training samples: 473\nNumber of validation samples: 60\nNumber of test samples: 59\nTotal number of samples: 592\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "X = df[[\"Hippocampus\", \"AGE\", \"Ventricles\"]]\n#X.insert(column=\"hipp_diff\" , value=(df[\"Hippocampus\"] - df[\"Hippocampus_bl\"]), loc=2)\n#X.insert(column=\"SEX\", value=(df[\"PTGENDER\"]==\"Male\"), loc=2)\n\ny = df[\"DX\"]\n\nX = X.reset_index(drop=True)\ny = y.reset_index(drop=True)",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from sklearn.preprocessing import StandardScaler",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scaler = StandardScaler()\nscaler.fit(X.loc[train_idx])",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "StandardScaler(copy=True, with_mean=True, with_std=True)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "classifier = SVC(C=10, kernel='poly')",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "classifier.fit(X=scaler.transform(X.loc[train_idx]), y=y.loc[train_idx])",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "SVC(C=10, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n    decision_function_shape='ovr', degree=3, gamma='scale', kernel='poly',\n    max_iter=-1, probability=False, random_state=None, shrinking=True,\n    tol=0.001, verbose=False)"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "scaler.transform(X.loc[train_idx])",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 15,
          "data": {
            "text/plain": "array([[-0.08525704, -2.5468523 ,  0.23374712],\n       [ 0.46067324, -0.73130483, -1.13330459],\n       [-1.11172159,  1.52299995, -0.80895342],\n       ...,\n       [ 0.16151356,  0.88755834, -0.35729376],\n       [-0.63732949,  1.17502002, -0.83601901],\n       [ 0.40828417, -2.75866617, -0.85659059]])"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# Training prediction\ny_pred = classifier.predict(X=scaler.transform(X.loc[train_idx]))\nprint(balanced_accuracy_score(y_true=y.loc[train_idx], y_pred=y_pred))\n\n# Validation prediction\ny_pred = classifier.predict(X=scaler.transform(X.loc[validation_idx]))\nprint(balanced_accuracy_score(y_true=y.loc[validation_idx], y_pred=y_pred))",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": "0.9012849380326771\n0.8\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "## Artificial Neural Network\n\n_This is an unfinished example meant as an optional, advanced exercise_"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "from keras.models import Sequential\nfrom keras.layers import Dense, Conv2D, MaxPooling2D, Dropout\nfrom keras.regularizers import l2\nfrom keras.optimizers import Adam\nfrom keras.models import load_model\nfrom keras.callbacks import EarlyStopping",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": "/home/nbuser/anaconda3_420/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n  from ._conv import register_converters as _register_converters\nUsing TensorFlow backend.\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:455: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:456: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:457: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n/home/nbuser/anaconda3_420/lib/python3.5/site-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def SimpleNet(drop_rate=0., weight_dcay=0.):\n    model = Sequential()\n\n    model.add(Dense(units=8, activation='relu', kernel_regularizer=l2(weight_dcay)))\n    model.add(Dense(units=1, activation='sigmoid'))\n    return model",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "model = SimpleNet()\nopti = Adam(lr=lr, decay=lr_decay)\nmodel.compile(optimizer=opti, loss='binary_crossentropy', metrics=['accuracy'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}