{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tutors: Fabian Eitel (Fabian.Eitel@charite.de) and Talia Kimber"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Aims of this session\n",
    "\n",
    "Get a rough idea of how artifical neural networks (ANNs) work, how an implementation in Keras looks like and how suitable they are for tabular data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning goals\n",
    "\n",
    "\n",
    "## Theory\n",
    "\n",
    "* Building blocks of ANNs\n",
    "* Model training\n",
    "\n",
    "## Practical\n",
    "\n",
    "* Learn to understand the basics using the Tensorflow playground\n",
    "* Learn to read a model defintion in Python using Keras\n",
    "* Run a pipeline of an ANN on the ADNI tabular data\n",
    "* Investigate what filters learn at different layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* Stanford Course on Deep Learning http://cs231n.github.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory\n",
    "\n",
    "\n",
    "### Building blocks of artificial neural networks\n",
    "Showing some of the blocks that can be used when training neural networks and some widely used examples.\n",
    "\n",
    "__Layer types__\n",
    "* Fully connected/linear/dense layers\n",
    "* Convolutional layers\n",
    "* Pooling layers and other down/upsampling layers\n",
    "* Utility layers like input and output layers\n",
    "* Batch normalization\n",
    "\n",
    "__Activation types__\n",
    "* Sigmoid\n",
    "* Linear\n",
    "* Tanh\n",
    "* ReLU\n",
    "* Leaky ReLU and other variants\n",
    "\n",
    "__Regularizers__\n",
    "* L1 regularization (used in LASSO)\n",
    "* L2 regularization / almost the same as weight decay (used in Ridge regression)\n",
    "* Dropout\n",
    "* Early stopping\n",
    "\n",
    "__Data functions__\n",
    "* Normalization (e.g. using mean and standard deviation)\n",
    "* Data augmentation\n",
    "* Feature reduction (e.g. Principal Component Analysis [PCA])\n",
    "\n",
    "__Cost functions__\n",
    "Cost functions depend on your type of analysis, i.e. regression, binary classification, multi-class classification etc.\n",
    "* Softmax\n",
    "* Cross-entropy\n",
    "* Binary cross-entropy\n",
    "* Kullback-Leibler Divergence\n",
    "* Smooth losses\n",
    "* Mean-squared error\n",
    "\n",
    "For more information on each topic view the course link in the references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scs.ryerson.ca/~aharley/vis/conv/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=AgkfIQ4IGaM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Playground excersises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Introduction__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://playground.tensorflow.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow playground is a neural network framework you can use in your browser. Unlike the name says its not based on the popular Tensorflow program. It allows to get some intuition on neural network workings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.1 Exercise__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the XOR dataset with 1 hidden layer and try out different loss functions:\n",
    "\n",
    "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4&seed=0.82689&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.2 Exercise__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if you add more features one by one?\n",
    "Start with X12. Maybe you can add extra layers and neurons too.\n",
    "\n",
    "https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=0&networkShape=4&seed=0.82689&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=true&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.3 Exercise__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try a different dataset. Investigate the effects of the learning rate on the training results:\n",
    "\n",
    "https://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=circle&regDataset=reg-plane&learningRate=0.001&regularizationRate=0&noise=0&networkShape=4,2&seed=0.19504&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Real data is never this clean, it is usually were noisy. Now, use the same model from above and add some noise to the data distribution (middle slider on the bottom left). How does it affect the data (you see it on the right) and your model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.4 Exercise__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you have added the noise, try out L1 and L2 regularization. When does it help?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.5 Exercise__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does it mean for a model to _converge_? Here is an example where it does not converge. Can you fix it?\n",
    "\n",
    "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.3&regularizationRate=0&noise=25&networkShape=4,2&seed=0.84469&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2.6 Exercise__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use everything you have learned so far on the more challenging spiral data:\n",
    "\n",
    "https://playground.tensorflow.org/#activation=tanh&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0&noise=25&networkShape=4,2&seed=0.07992&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false\n",
    "\n",
    "Can you beat my quick experiments?\n",
    "\n",
    "https://playground.tensorflow.org/#activation=relu&regularization=L2&batchSize=10&dataset=spiral&regDataset=reg-plane&learningRate=0.03&regularizationRate=0.03&noise=25&networkShape=5,4,2&seed=0.16124&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=true&xSquared=true&ySquared=true&cosX=false&sinX=true&cosY=false&sinY=true&collectStats=false&problem=classification&initZero=false&hideText=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.metrics import balanced_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fabiane/anaconda2/envs/postal/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3018: DtypeWarning: Columns (16,17,18,99,100) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RID</th>\n",
       "      <th>VISCODE</th>\n",
       "      <th>SITE</th>\n",
       "      <th>EXAMDATE</th>\n",
       "      <th>DX_bl</th>\n",
       "      <th>AGE</th>\n",
       "      <th>PTGENDER</th>\n",
       "      <th>PTEDUCAT</th>\n",
       "      <th>WORK</th>\n",
       "      <th>PTETHCAT</th>\n",
       "      <th>...</th>\n",
       "      <th>TAU_bl</th>\n",
       "      <th>PTAU_bl</th>\n",
       "      <th>FDG_bl</th>\n",
       "      <th>PIB_bl</th>\n",
       "      <th>AV45_bl</th>\n",
       "      <th>Years_bl</th>\n",
       "      <th>Month_bl</th>\n",
       "      <th>Month</th>\n",
       "      <th>M</th>\n",
       "      <th>update_stamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>bl</td>\n",
       "      <td>11</td>\n",
       "      <td>2005-09-08</td>\n",
       "      <td>CN</td>\n",
       "      <td>74.3</td>\n",
       "      <td>Male</td>\n",
       "      <td>16</td>\n",
       "      <td>technical writer and editor</td>\n",
       "      <td>Not Hisp/Latino</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.36665</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-12-04 04:19:56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>bl</td>\n",
       "      <td>11</td>\n",
       "      <td>2005-09-12</td>\n",
       "      <td>AD</td>\n",
       "      <td>81.3</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Secretary</td>\n",
       "      <td>Not Hisp/Latino</td>\n",
       "      <td>...</td>\n",
       "      <td>239.7</td>\n",
       "      <td>22.83</td>\n",
       "      <td>1.08355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2019-12-04 04:19:56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>m06</td>\n",
       "      <td>11</td>\n",
       "      <td>2006-03-13</td>\n",
       "      <td>AD</td>\n",
       "      <td>81.3</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Elementary school teacher</td>\n",
       "      <td>Not Hisp/Latino</td>\n",
       "      <td>...</td>\n",
       "      <td>239.7</td>\n",
       "      <td>22.83</td>\n",
       "      <td>1.08355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.498289</td>\n",
       "      <td>5.96721</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2019-12-04 04:19:56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>m12</td>\n",
       "      <td>11</td>\n",
       "      <td>2006-09-12</td>\n",
       "      <td>AD</td>\n",
       "      <td>81.3</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Communication</td>\n",
       "      <td>Not Hisp/Latino</td>\n",
       "      <td>...</td>\n",
       "      <td>239.7</td>\n",
       "      <td>22.83</td>\n",
       "      <td>1.08355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999316</td>\n",
       "      <td>11.96720</td>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2019-12-04 04:19:56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>m24</td>\n",
       "      <td>11</td>\n",
       "      <td>2007-09-12</td>\n",
       "      <td>AD</td>\n",
       "      <td>81.3</td>\n",
       "      <td>Male</td>\n",
       "      <td>18</td>\n",
       "      <td>Accounting</td>\n",
       "      <td>Not Hisp/Latino</td>\n",
       "      <td>...</td>\n",
       "      <td>239.7</td>\n",
       "      <td>22.83</td>\n",
       "      <td>1.08355</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.998630</td>\n",
       "      <td>23.93440</td>\n",
       "      <td>24</td>\n",
       "      <td>24.0</td>\n",
       "      <td>2019-12-04 04:19:56.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 109 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   RID VISCODE  SITE    EXAMDATE DX_bl   AGE PTGENDER  PTEDUCAT  \\\n",
       "0    2      bl    11  2005-09-08    CN  74.3     Male        16   \n",
       "1    3      bl    11  2005-09-12    AD  81.3     Male        18   \n",
       "2    3     m06    11  2006-03-13    AD  81.3     Male        18   \n",
       "3    3     m12    11  2006-09-12    AD  81.3     Male        18   \n",
       "4    3     m24    11  2007-09-12    AD  81.3     Male        18   \n",
       "\n",
       "                          WORK         PTETHCAT          ...           TAU_bl  \\\n",
       "0  technical writer and editor  Not Hisp/Latino          ...              NaN   \n",
       "1                    Secretary  Not Hisp/Latino          ...            239.7   \n",
       "2    Elementary school teacher  Not Hisp/Latino          ...            239.7   \n",
       "3                Communication  Not Hisp/Latino          ...            239.7   \n",
       "4                   Accounting  Not Hisp/Latino          ...            239.7   \n",
       "\n",
       "  PTAU_bl   FDG_bl  PIB_bl  AV45_bl  Years_bl  Month_bl Month     M  \\\n",
       "0     NaN  1.36665     NaN      NaN  0.000000   0.00000     0   0.0   \n",
       "1   22.83  1.08355     NaN      NaN  0.000000   0.00000     0   0.0   \n",
       "2   22.83  1.08355     NaN      NaN  0.498289   5.96721     6   6.0   \n",
       "3   22.83  1.08355     NaN      NaN  0.999316  11.96720    12  12.0   \n",
       "4   22.83  1.08355     NaN      NaN  1.998630  23.93440    24  24.0   \n",
       "\n",
       "            update_stamp  \n",
       "0  2019-12-04 04:19:56.0  \n",
       "1  2019-12-04 04:19:56.0  \n",
       "2  2019-12-04 04:19:56.0  \n",
       "3  2019-12-04 04:19:56.0  \n",
       "4  2019-12-04 04:19:56.0  \n",
       "\n",
       "[5 rows x 109 columns]"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data table\n",
    "df = pd.read_csv(\"alzheimers_disease.csv\")\n",
    "# Print first 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RID',\n",
       " 'VISCODE',\n",
       " 'SITE',\n",
       " 'EXAMDATE',\n",
       " 'DX_bl',\n",
       " 'AGE',\n",
       " 'PTGENDER',\n",
       " 'PTEDUCAT',\n",
       " 'WORK',\n",
       " 'PTETHCAT',\n",
       " 'PTRACCAT',\n",
       " 'PTMARRY',\n",
       " 'APOE4',\n",
       " 'FDG',\n",
       " 'PIB',\n",
       " 'AV45',\n",
       " 'ABETA',\n",
       " 'TAU',\n",
       " 'PTAU',\n",
       " 'CDRSB',\n",
       " 'ADAS11',\n",
       " 'ADAS13',\n",
       " 'ADASQ4',\n",
       " 'MMSE',\n",
       " 'RAVLT_immediate',\n",
       " 'RAVLT_learning',\n",
       " 'RAVLT_forgetting',\n",
       " 'RAVLT_perc_forgetting',\n",
       " 'LDELTOTAL',\n",
       " 'DIGITSCOR',\n",
       " 'TRABSCOR',\n",
       " 'FAQ',\n",
       " 'MOCA',\n",
       " 'EcogPtMem',\n",
       " 'EcogPtLang',\n",
       " 'EcogPtVisspat',\n",
       " 'EcogPtPlan',\n",
       " 'EcogPtOrgan',\n",
       " 'EcogPtDivatt',\n",
       " 'EcogPtTotal',\n",
       " 'EcogSPMem',\n",
       " 'EcogSPLang',\n",
       " 'EcogSPVisspat',\n",
       " 'EcogSPPlan',\n",
       " 'EcogSPOrgan',\n",
       " 'EcogSPDivatt',\n",
       " 'EcogSPTotal',\n",
       " 'FLDSTRENG',\n",
       " 'IMAGEUID',\n",
       " 'Ventricles',\n",
       " 'Hippocampus',\n",
       " 'WholeBrain',\n",
       " 'Entorhinal',\n",
       " 'Fusiform',\n",
       " 'MidTemp',\n",
       " 'ICV',\n",
       " 'DX',\n",
       " 'mPACCdigit',\n",
       " 'mPACCtrailsB',\n",
       " 'EXAMDATE_bl',\n",
       " 'CDRSB_bl',\n",
       " 'ADAS11_bl',\n",
       " 'ADAS13_bl',\n",
       " 'ADASQ4_bl',\n",
       " 'MMSE_bl',\n",
       " 'RAVLT_immediate_bl',\n",
       " 'RAVLT_learning_bl',\n",
       " 'RAVLT_forgetting_bl',\n",
       " 'RAVLT_perc_forgetting_bl',\n",
       " 'LDELTOTAL_BL',\n",
       " 'DIGITSCOR_bl',\n",
       " 'TRABSCOR_bl',\n",
       " 'FAQ_bl',\n",
       " 'mPACCdigit_bl',\n",
       " 'mPACCtrailsB_bl',\n",
       " 'FLDSTRENG_bl',\n",
       " 'Ventricles_bl',\n",
       " 'Hippocampus_bl',\n",
       " 'WholeBrain_bl',\n",
       " 'Entorhinal_bl',\n",
       " 'Fusiform_bl',\n",
       " 'MidTemp_bl',\n",
       " 'ICV_bl',\n",
       " 'MOCA_bl',\n",
       " 'EcogPtMem_bl',\n",
       " 'EcogPtLang_bl',\n",
       " 'EcogPtVisspat_bl',\n",
       " 'EcogPtPlan_bl',\n",
       " 'EcogPtOrgan_bl',\n",
       " 'EcogPtDivatt_bl',\n",
       " 'EcogPtTotal_bl',\n",
       " 'EcogSPMem_bl',\n",
       " 'EcogSPLang_bl',\n",
       " 'EcogSPVisspat_bl',\n",
       " 'EcogSPPlan_bl',\n",
       " 'EcogSPOrgan_bl',\n",
       " 'EcogSPDivatt_bl',\n",
       " 'EcogSPTotal_bl',\n",
       " 'ABETA_bl',\n",
       " 'TAU_bl',\n",
       " 'PTAU_bl',\n",
       " 'FDG_bl',\n",
       " 'PIB_bl',\n",
       " 'AV45_bl',\n",
       " 'Years_bl',\n",
       " 'Month_bl',\n",
       " 'Month',\n",
       " 'M',\n",
       " 'update_stamp']"
      ]
     },
     "execution_count": 474,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df.VISCODE == \"m12\"]\n",
    "df = df[df.DX != \"MCI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df.dropna(subset=[\"Hippocampus\", \"DX\", \"Ventricles\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Order before shuffling: [0 1 2 3 4]\n",
      "Order after shuffling: [505 586 195 165 453]\n"
     ]
    }
   ],
   "source": [
    "# Get an array with the number of samples\n",
    "indices = np.arange(len(df))\n",
    "print(\"Order before shuffling: %s\" % indices[:5])\n",
    "\n",
    "# Shuffle that array\n",
    "np.random.seed(42) # fix a seed so each random event can be repeated\n",
    "np.random.shuffle(indices)\n",
    "print(\"Order after shuffling: %s\"  % indices[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples for training set: 472\n"
     ]
    }
   ],
   "source": [
    "# Take first 80% as a training set\n",
    "len_training = int(len(indices) * 0.8) # use int() function to remove decimals\n",
    "print(\"Number of samples for training set: %i\" % len_training)\n",
    "\n",
    "# Select the first 80% indices\n",
    "train_idx = indices[0:len_training] # pick 0 to the value of len_training from the indices array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Take the remaining data and split it 50/50\n",
    "remaining_samples = len(indices) - len_training\n",
    "len_validation = int(np.ceil(remaining_samples/2)) # round up once\n",
    "len_test = int(np.floor(remaining_samples/2)) # round down once\n",
    "\n",
    "# Select from the indices array the individual groups\n",
    "validation_idx = indices[len_training:len_training+len_validation]\n",
    "test_idx = indices[len_training+len_validation:len(indices)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 472\n",
      "Number of validation samples: 60\n",
      "Number of test samples: 59\n",
      "Total number of samples: 591\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training samples: %i\" % len(train_idx))\n",
    "print(\"Number of validation samples: %i\" % len(validation_idx))\n",
    "print(\"Number of test samples: %i\" % len(test_idx))\n",
    "print(\"Total number of samples: %i\" % (len(train_idx) + len(validation_idx) + len(test_idx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[[\"Hippocampus\", \"AGE\", \"Ventricles\"]]\n",
    "#X.insert(column=\"hipp_diff\" , value=(df[\"Hippocampus\"] - df[\"Hippocampus_bl\"]), loc=2)\n",
    "#X.insert(column=\"SEX\", value=(df[\"PTGENDER\"]==\"Male\"), loc=2)\n",
    "\n",
    "y = df[\"DX\"]\n",
    "\n",
    "X = X.reset_index(drop=True)\n",
    "y = y.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 483,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X.loc[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classifier = SVC(C=10, kernel='poly')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='poly', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 505,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X=scaler.transform(X.loc[train_idx]), y=y.loc[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.373735  , -0.37634159,  0.78155586],\n",
       "       [ 1.05409896, -0.78490257, -1.13213411],\n",
       "       [-2.24543897,  1.56054006, -0.80227614],\n",
       "       ...,\n",
       "       [ 0.42298183,  1.03092398, -0.34336257],\n",
       "       [-1.25065291,  1.36382552, -0.82995622],\n",
       "       [ 0.93817949, -2.87310311, -0.85099308]])"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(X.loc[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8340873692798338\n",
      "0.8919413919413919\n"
     ]
    }
   ],
   "source": [
    "# Training prediction\n",
    "y_pred = classifier.predict(X=scaler.transform(X.loc[train_idx]))\n",
    "print(balanced_accuracy_score(y_true=y.loc[train_idx], y_pred=y_pred))\n",
    "\n",
    "# Validation prediction\n",
    "y_pred = classifier.predict(X=scaler.transform(X.loc[validation_idx]))\n",
    "print(balanced_accuracy_score(y_true=y.loc[validation_idx], y_pred=y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D MaxPooling2D, Dropout\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SimpleNet(input_shape, drop_rate=0., weight_dcay=0.):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(units=8, activation='relu', kernel_regularizer=l2(weight_dcay)))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = SimpleNet()\n",
    "opti = Adam(lr=lr, decay=lr_decay)\n",
    "model.compile(optimizer=opti, loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (postal)",
   "language": "python",
   "name": "postal"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
